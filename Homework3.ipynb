{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorWithPadding\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#device configuration\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text_files():#data Preparation and Loading\n",
    "    all_sentences = []\n",
    "    \n",
    "    if not os.path.exists('dataset'):#create dataset directory if it doesn't exist\n",
    "        os.makedirs('dataset')\n",
    "        \n",
    "    source_files = [#move files to dataset directory if they exist in current directory\n",
    "        'finetuning_data1.txt',\n",
    "        'finetuning_data2.txt',\n",
    "        'finetuning_data3.txt',\n",
    "        'finetuning_data4.txt',\n",
    "        'finetuning_data5.txt'\n",
    "    ]\n",
    "    \n",
    "    current_dir_files = os.listdir()# first we check if files exist in current directory\n",
    "    print(\"Files found in current directory:\", [f for f in current_dir_files if f.endswith('.txt')])\n",
    "    \n",
    "    for file in source_files: # if file exists in current directory\n",
    "        if file in current_dir_files:#if file exists in current directory, copy it to dataset directory\n",
    "            with open(file, 'r', encoding='utf-8') as source:\n",
    "                content = source.read()\n",
    "            with open(os.path.join('dataset', file), 'w', encoding='utf-8') as dest:\n",
    "                dest.write(content)\n",
    "            print(f\"Copied {file} to dataset directory\")\n",
    "    \n",
    "    dataset_files = glob.glob('dataset/*.txt')#read and combine all files from dataset directory\n",
    "    if not dataset_files:\n",
    "        raise FileNotFoundError(\"No text files found in the dataset directory!\")\n",
    "    \n",
    "    print(\"\\nProcessing files:\")\n",
    "    for file_path in dataset_files: # read and combine all files\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            sentences = [line.strip() for line in file if line.strip()]\n",
    "            all_sentences.extend(sentences)# combine all sentences\n",
    "            print(f\"- {os.path.basename(file_path)}: {len(sentences)} sentences\")\n",
    "    \n",
    "    with open('combined_text_data.txt', 'w', encoding='utf-8') as f:#write combined text to a single file\n",
    "        f.write('\\n'.join(all_sentences))\n",
    "    \n",
    "    print(f\"\\nTotal number of sentences: {len(all_sentences)}\")\n",
    "    print(\"First few sentences as sample:\")\n",
    "    for i in range(min(3, len(all_sentences))):# print first 3 sentences\n",
    "        print(f\"{i+1}. {all_sentences[i]}\")\n",
    "    \n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "try:#load the data\n",
    "    combined_text = combine_text_files()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n",
    "    print(\"\\nPlease ensure your text files are in the same directory as the notebook\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):#custom dataset class\n",
    "    def __init__(self, tokenizer, text_data, max_length=256):  #increased max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_data = text_data\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):# return the length of the dataset\n",
    "        return len(self.text_data)\n",
    "    \n",
    "    def __getitem__(self, idx):# get the item at index\n",
    "        text = self.text_data[idx]# get the text at index\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            add_special_tokens=True# explicitly add special tokens\n",
    "        )\n",
    "        \n",
    "        position_ids = torch.arange(0, len(encodings['input_ids'][0])).unsqueeze(0)#add position IDs\n",
    "        \n",
    "        return {# return the input IDs, attention mask, labels, and position IDs\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': encodings['input_ids'].squeeze(),\n",
    "            'position_ids': position_ids.squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model_and_tokenizer():# model and tokenizer \n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')#load tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')#load model\n",
    "    \n",
    "    if tokenizer.pad_token is None:#add padding token if not set\n",
    "        tokenizer.pad_token = tokenizer.eos_token# set padding token\n",
    "        model.config.pad_token_id = model.config.eos_token_id# set padding token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = setup_model_and_tokenizer()#setup model and tokenizer\n",
    "model = model.to(device)#move model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = CustomDataset(tokenizer, combined_text)#prepare training dataset\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(#training arguments\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    num_train_epochs=10, \n",
    "    per_device_train_batch_size=2, \n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    fp16=True if torch.cuda.is_available() else False, #enable mixed precision training if GPU available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(#initialize Trainer and Train\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()#start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./gpt2-finetuned-final\")#save Model\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned-final\")#save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()#ensure your model is in evaluation mode to disable dropout layers\n",
    "\n",
    "#define prompts and target words\n",
    "prompts = [\"Michael loved sitting on his\", \"Sofia enjoyed listening to her\", \"Bristi's favorite subject in school was\", \"Krish loved puzzles and could spend hours solving\", \"Rabbi's favorite subject in school was\"]\n",
    "target_words = [\"porch\", \"grandfather\", \"art\", \"jigsaw\", \"science\"]\n",
    "\n",
    "num_generations = 50#set the number of generations per prompt\n",
    "min_count = 30#set the threshold for the minimum count of target words\n",
    "\n",
    "def check_target_word_occurrence(prompt, target_word, num_generations, min_count):#function to check occurrences of target words in generated texts\n",
    "    count = 0\n",
    "    for _ in range(num_generations):#generate text num_generations times\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids #tokenize the prompt text and convert to tensor\n",
    "        attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask #get attention mask\n",
    "        input_ids = input_ids.to(device)#move input_ids and attention_mask tensor to GPU if available\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        output = model.generate( #generate text from the model\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            max_length=100,\n",
    "            num_beams=10,           \n",
    "            temperature=0.8,        \n",
    "            top_k=40,              \n",
    "            top_p=0.9,             \n",
    "            do_sample=True,        \n",
    "            repetition_penalty=1.2, \n",
    "            no_repeat_ngram_size=2, \n",
    "            early_stopping=True,    \n",
    "            length_penalty=1.0,     \n",
    "            min_length=20          \n",
    "        )\n",
    "\n",
    "        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)#decode the generated text back to string\n",
    "        if target_word in generated_text: #check if the target word appears in the generated text\n",
    "            count += 1\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, target_word in zip(prompts, target_words):#iterate over each prompt and target word\n",
    "    count = check_target_word_occurrence(prompt, target_word, num_generations, min_count)#check occurrences of target word\n",
    "    print(f\"Prompt: '{prompt}' | Target Word: '{target_word}' | Count: {count}\")\n",
    "\n",
    "    if count >= min_count:#check if the count meets the minimum threshold\n",
    "        print(f\"The target word '{target_word}' appeared at least {min_count} times.\")\n",
    "    else:#if the count does not meet the minimum threshold\n",
    "        print(f\"The target word '{target_word}' appeared less than {min_count} times.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
